version: 2.1

orbs:
  aws-eks: circleci/aws-eks@0.2.0
  aws-ecr: circleci/aws-ecr@3.1.0
  kubernetes: circleci/kubernetes@0.3.0
  docker: circleci/docker@1.6.0

workflows:
  deployment:
    jobs:
      - check-docker-code
      - docker/publish:
          image: $DOCKER_LOGIN/udacity-capstone-app-deployment
          path: ~/project/nginx_app
          update-description: true
          docker-context: ~/project/nginx_app
          tag: ${CIRCLE_SHA1:0:7},latest
          requires:
            - check-docker-code
      - create-kubernetes-cluster:
          requires:
            - docker/publish
      - smoke-test:
          requires:
            - create-kubernetes-cluster
      

jobs:
    check-docker-code:
      docker:
      - image: python:3.7.3-stretch
      working_directory: ~/project/
      steps:
      - checkout
      - run:
          name: Installing Linters
          command: |
            cd nginx_app
            make install
      - run:
          name: run lint on Dockerfile
          command: |
            cd nginx_app          
            make lint
            RESULT=$?
            if [ ${RESULT} -eq 0 ]; then
              echo "successfully linted files - proceeding to build image"
            else
              echo "failed to lint files - please fix and try again"
            fi

    create-kubernetes-cluster:
      docker:
        - image: alpine/k8s:1.15.12
      working_directory: /tmp/workspace
      steps:
        - checkout
        - run:
            name: store current cluster name
            command: |
              echo $(aws eks list-clusters | python3 -c "import sys, json; print(json.load(sys.stdin)['clusters'][0])") > cluster.txt
              cat cluster.txt

        - run:
            name: create kubernetes cluster
            command: |
              eksctl create cluster --name="udacity-capstone-${CIRCLE_SHA1:0:7}" \
              --region eu-west-1 \
              --with-oidc \
              --ssh-access \
              --ssh-public-key poppy-key 
        - run:
            name: store current cluster name
            command: |
              echo $(aws eks list-clusters | python3 -c "import sys, json; print(json.load(sys.stdin)['clusters'][0])") > ~/cluster.txt
              cat ~/cluster.txt
        - run:
            name: create kubernetes resources
            command: |
              aws eks update-kubeconfig --name "udacity-capstone-${CIRCLE_SHA1:0:7}"
              export KUBECONFIG=/root/.kube/config
              kubectl get svc
              kubectl apply -f deployment/
              kubectl rollout status deployment nginx-web-app

        - run:
            name: add backend service ip to a file
            command: |
              echo "$(kubectl get services nginx-web-app-service --output jsonpath='{.status.loadBalancer.ingress[0].hostname}')" > ~/service.txt
              cat ~/service.txt

        - persist_to_workspace:
            root: ~/
            paths:
              - cluster.txt
              - service.txt

    smoke-test:
        docker:
          - image: python:3.7-alpine3.11
        steps:
          - checkout
          - attach_workspace:
              at: ~/
          - run:
              name: Install dependencies
              command: |
                apk add --update curl
          - run:
              name: smoke test
              command: |

                export URL=$(cat project/service.txt)"15444/hello"

                if curl -s ${URL} | grep "Hello World"
                then
                  echo "Smoke Test Success"
                  return 0  
                else
                  echo "Unable to access Website"
                  return 1
                fi
          
#         - destroy-environment


# commands:
#   destroy-environment:
#     description: Destroy backend and frontend stacks given a workflow ID.
#     steps:
#       - run:
#           name: Destroy Backend Infrastructure
#           when: on_fail
#           command: aws cloudformation delete-stack --stack-name backend-${CIRCLE_WORKFLOW_ID:0:7}

